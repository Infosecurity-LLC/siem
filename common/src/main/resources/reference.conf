app {
  streaming.batch.duration = 10 seconds
  spark {
    //    app.name="Streamer"
    //    master="local[2]"
    //    metrics.namespace="siem_"
    streaming.backpressure.enabled = true
    streaming.kafka.maxRatePerPartition = 10000
  }
  kafka {
    input {
      //      bootstrap.servers = "localhost:60000"
      //      group.id = "normalizer"
      //      topics = []
      key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
      value.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
      auto.offset.reset = "latest",
      enable.auto.commit = false
    }
    output {
      write.timeout = 10 seconds
      basic.producer.settings {
        //     master only
        acks = "0"
        retries = 0
        batch.size = 65535
        linger.ms = 1000
        //              128 MB
        buffer.memory = 134217728
        key.serializer = "org.apache.kafka.common.serialization.StringSerializer"
        value.serializer = "org.apache.kafka.common.serialization.ByteArraySerializer"
      }
      //      topic.mappings {
      //        // message state to topic name
      //        normalized = "normalized"
      //        error = "error"
      //        invalid = "invalid"
      //        raw = "raw"
      //        chain = "chain"
      //        enriched = "enricher_to_prefilter"
      //      }
    }
  }
  streamers_meta {
    connectionTestQuery = "select 1"
    poolName = "streamers_meta"
    maximumPoolSize = 3
    idleTimeout = 60000
    //    jdbcUrl = "jdbc:postgresql://host:port/database"
    //    username =
    //    password =
    schema = public
  }
  elastic {
    // if SocEventExtractor selected @timestamp leads to SocEvent's `data.time` field
    es.resource.write = "{eventSource.title}-{@timestamp|dd.MM.yyyy}/normalized"
    es.mapping.default.extractor.class = "ru.gkis.soc.siem.io.elastic.SocEventExtractor"
    es.mapping.index.formatter.class = "ru.gkis.soc.siem.io.elastic.IndexDateFormatter"
    es.index.auto.create = true
    es.mapping.id = "id"
    //    es.nodes =
    //    es.port =
  }
  normalizer {
    //  available modes:
    //  * `as-is` - does nothing
    //  * `now` - sets `data.time` to current timestamp
    //  * `shift` - shifts `data.time` by time.shift.value
    time.shift.mode = "as-is"
    time.shift.value = 0
  }
  hbase {
    //    site.xml.url = "file:///....../hbase-site.xml"
    //    core.xml.url = "hdfs:///....../core-site.xml"
    namespace = "isoc"
    //    table.mappings {
    //      // topic to table mapping
    //      // normalized should be raplaced to enriched later on
    //      normalized = "normalized"
    //      raw = "raw"
    //      chain = "chain"
    //    }

    // Archiver hbase settings
    //    name - table name from which we want to archive data
    //    eventType - Type of an event we want to archive. Available options: "RawEvent", "ChainEvent", "SocEvent"
    //    partitionColumns - either ["collector_organization", "eventTime"] for raw and chain
    //    or ["collector_organization", "data_time"] for soc events
    //    table {
    //      name =
    //      eventType =
    //      partitionColumns {
    //         org = "collector_organization"
    //         date = "data_time"
    //       }
    //    }
    column.family = "n"
    time.column = "t"
    event.column = "e"
    organization.column = "o"
    connection.parallelism = 2
    write.timeout = 10 seconds

    //    kerberos {
    //      principal = ""
    //      keytab = ""
    //    }
  }
  archiver {
    //    outputDir =
    archiverRecordsTTL = 30 d
    saveMode = Append
  }

  hive {
    //    metastore.uris = "thrift://sandbox-hdp.hortonworks.com:10016"
    //    tableName = "test"
  }

  enricher {
    cache.geoIp.updateInterval = "24 hour"
    cache.schedule.updateInterval = "10 m"
    cache.logins.updateInterval = "10 m"
    cache.destinationHostRules.updateInterval = "10 m"
    cache.windowsObjectAccessRules.updateInterval = "10 m"
    cache.vpnLogonRules.updateInterval = "10 m"
    cache.firewallConnectionRules.updateInterval = "10 m"
    cache.proizvodstvennyyKalendar.updateInterval = "24 hour"
  }

  filter {
    cache.scripts.updateInterval = "10 m"
  }
}
